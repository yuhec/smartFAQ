{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00000-85f19fe7-3c67-4a7f-adb0-aefba0bfb102",
        "deepnote_cell_type": "code"
      },
      "source": "import pickle\nimport tensorflow as tf\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Input, Embedding, LSTM, Dense, Activation, Conv1D, Flatten, Dropout\n\nfrom tensorflow.keras.callbacks import TensorBoard\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nimport os",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Load w2v model & Data",
      "metadata": {
        "cell_id": "00001-19f79011-c3dc-48c5-b5f4-95208f3a8d42",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00002-0c1d4b99-c4cd-4a38-8c08-67c5c206fb61",
        "deepnote_cell_type": "code"
      },
      "source": "# with open('../../data/w2v_pretrained_weights.pickle', 'rb') as handle:\n#     w2v_model = pickle.load(handle)\nwith open('../../data/x_train.pickle', 'rb') as handle:\n    X_train = pickle.load(handle)\nwith open('../../data/y_train.pickle', 'rb') as handle:\n    y_train = pickle.load(handle)\nwith open('../../data/x_val.pickle', 'rb') as handle:\n    X_val = pickle.load(handle)\nwith open('../../data/y_val.pickle', 'rb') as handle:\n    y_val = pickle.load(handle)",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00003-774ef9ae-d265-44ba-89a6-9183c94d3201",
        "deepnote_cell_type": "code"
      },
      "source": "# pretrained_weights = w2v_model.wv.syn0\n# vocab_size, embedding_size = pretrained_weights.shape",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Tokenization & Padding",
      "metadata": {
        "cell_id": "00004-bc46d2ac-d6aa-46a3-b64a-30c8c82cf215",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00005-60838086-5f2d-4f25-8301-86440eacd0dc",
        "deepnote_cell_type": "code"
      },
      "source": "MAX_NUM_WORDS = 20000",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00006-2326129b-d6e3-4963-bcc4-6605675620fe",
        "deepnote_cell_type": "code"
      },
      "source": "input_tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\ninput_tokenizer.fit_on_texts(X_train['answer'])\ninput_integer_seq = input_tokenizer.texts_to_sequences(X_train['answer'])\n\nword2idx_inputs = input_tokenizer.word_index\nprint('Total unique words in the input: %s' % len(word2idx_inputs))\n\nmax_input_len = max(len(sen) for sen in input_integer_seq)\nprint(\"Length of longest sentence in input: %g\" % max_input_len)",
      "execution_count": 5,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Total unique words in the input: 30369\nLength of longest sentence in input: 1688\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00007-f29f8a76-fc16-4fb0-95e4-98ccdf0b83b2",
        "deepnote_cell_type": "code"
      },
      "source": "x_train_pad = pad_sequences(input_integer_seq, maxlen=max_input_len)\n# x_train_pad = pad_sequences(input_integer_seq, maxlen=MAX_SEQ_LEN)\nprint(\"encoder_input_sequences.shape:\", x_train_pad.shape)\nprint(\"encoder_input_sequences[172]:\", x_train_pad[72])",
      "execution_count": 6,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "encoder_input_sequences.shape: (10000, 1688)\nencoder_input_sequences[172]: [  0   0   0 ... 135 183  86]\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00008-4da338c3-5148-4871-b0c3-cfc254039ee3",
        "deepnote_cell_type": "code"
      },
      "source": "input_tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\ninput_tokenizer.fit_on_texts(X_val['answer'])\ninput_integer_seq = input_tokenizer.texts_to_sequences(X_val['answer'])\nx_val_pad = pad_sequences(input_integer_seq, maxlen=max_input_len)\nprint(\"encoder_input_sequences.shape:\", x_val_pad.shape)\nprint(\"encoder_input_sequences[172]:\", x_val_pad[72])",
      "execution_count": 7,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "encoder_input_sequences.shape: (10000, 1688)\nencoder_input_sequences[172]: [   0    0    0 ... 5837 5419 5838]\n"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "## Keras Model",
      "metadata": {
        "cell_id": "00009-33501c64-a236-423c-a6bd-cdc3b3ff399d",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00010-458ba6bc-007a-4c3d-ac09-7ecc26c1076d",
        "deepnote_cell_type": "code"
      },
      "source": "from gensim.models import Word2Vec\nfrom gensim.models.phrases import Phrases, Phraser",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00011-46e6bb7d-208a-4ac3-bdee-4e056c4e375f",
        "deepnote_cell_type": "code"
      },
      "source": "sentences = [[word for word in document.lower().split()] for document in X_train['answer']]",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00012-b79a5c35-459b-430f-ad52-80420c715240",
        "deepnote_cell_type": "code"
      },
      "source": "sentences",
      "execution_count": 17,
      "outputs": [
        {
          "data": {
            "text/plain": "<gensim.models.phrases.Phrases at 0x7fe792a07898>"
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00013-d89f4618-272f-4a50-81c3-69e2c794ccbb",
        "deepnote_cell_type": "code"
      },
      "source": "phrases = Phrases(sentences)\nbigram = Phraser(phrases)\nsentences = bigram[sentences]",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00014-dcb2c719-655c-4e21-a990-4f54a5b23763",
        "deepnote_cell_type": "code"
      },
      "source": "word_model = Word2Vec(sentences, size=200, min_count = 1, window = 5)",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00015-31b16ca3-3113-4a26-b487-b9ae333ac3c9",
        "deepnote_cell_type": "code"
      },
      "source": "# sentences",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00016-e4dad336-6753-4d52-b0da-d97cfa389b6d",
        "deepnote_cell_type": "code"
      },
      "source": "pretrained_weights = word_model.wv.syn0\nvocab_size, embedding_size = pretrained_weights.shape",
      "execution_count": 21,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n  \"\"\"Entry point for launching an IPython kernel.\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00017-800e1d13-4c58-4825-9d56-29483846c058",
        "deepnote_cell_type": "code"
      },
      "source": "def build_model(vocab_size,embedding_size,pretrained_weights):\n    \n           \n    model = tf.keras.Sequential()\n    \n    model.add(Embedding(input_dim=vocab_size, \n                        output_dim=embedding_size, \n                        weights=[pretrained_weights],\n                        input_length=max_input_len\n                       ))\n\n    model.add(Conv1D(128,5, activation='relu'))\n    model.add(Conv1D(128,5, activation='relu'))\n    \n    model.add(Flatten())\n    model.add(Dropout(.2, input_shape=(2,)))\n    \n    model.add(Dense(10, activation='relu'))\n    model.add(Activation('relu'))\n    model.add(Dense(1, activation='sigmoid'))\n\n    return model",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00018-175b5080-0fac-4127-babd-17cb36e2defd",
        "deepnote_cell_type": "code"
      },
      "source": "# MAE : the average absolute distance between the predicted and target values\ndef compile_model(model):\n    mae = tf.keras.losses.MeanAbsolutePercentageError()\n    model.compile(loss=mae,\n                  optimizer='adam',\n                  metrics=['mean_squared_error', 'mean_absolute_error']\n                  )\n    return  model",
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00019-268a1ff0-29ba-49d1-9404-8c75bb114130",
        "deepnote_cell_type": "code"
      },
      "source": "def fit_model( x_train, y_train, x_val, y_val, model, batch_size,  epochs = 5):\n    \n    print('Train...')\n    os.makedirs(\"./logs/CNN_logs\",exist_ok=True)\n    tensorboard = TensorBoard(log_dir=os.path.join('./logs/CNN_logs'), histogram_freq=0,\n                                  write_graph=True, write_images=False,profile_batch = 100000000)\n\n    # This callback will stop the training when there is no improvement in\n    # the validation loss for 2 consecutive epochs.\n    Es= EarlyStopping(monitor='loss', patience=2)\n    \n    callbacks = [Es, tensorboard]\n\n    model.fit(x_train, y_train,\n              batch_size=batch_size,\n              epochs=epochs,\n              validation_data=(x_val, y_val),\n              callbacks= callbacks)\n    return model",
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00020-a19ca4da-0270-4223-9581-e66aac8e6fc3",
        "deepnote_cell_type": "code"
      },
      "source": "with tf.device(\"/CPU:0\"):\n    model = build_model(vocab_size, embedding_size, pretrained_weights)\n    model.summary()\n    model = compile_model(model)\n    model = fit_model(x_train_pad, y_train, x_val_pad, y_val, model, batch_size=200, epochs=3)",
      "execution_count": 39,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Model: \"sequential_4\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_4 (Embedding)      (None, 1688, 200)         6710200   \n_________________________________________________________________\nconv1d_8 (Conv1D)            (None, 1684, 128)         128128    \n_________________________________________________________________\nconv1d_9 (Conv1D)            (None, 1680, 128)         82048     \n_________________________________________________________________\nflatten_4 (Flatten)          (None, 215040)            0         \n_________________________________________________________________\ndropout_4 (Dropout)          (None, 215040)            0         \n_________________________________________________________________\ndense_8 (Dense)              (None, 10)                2150410   \n_________________________________________________________________\nactivation_4 (Activation)    (None, 10)                0         \n_________________________________________________________________\ndense_9 (Dense)              (None, 1)                 11        \n=================================================================\nTotal params: 9,070,797\nTrainable params: 9,070,797\nNon-trainable params: 0\n_________________________________________________________________\nTrain...\nEpoch 1/3\n50/50 [==============================] - 167s 3s/step - loss: 26895181.2059 - mean_squared_error: 230.1529 - mean_absolute_error: 2.9629 - val_loss: 67.6000 - val_mean_squared_error: 147.6348 - val_mean_absolute_error: 2.9092\nEpoch 2/3\n50/50 [==============================] - 170s 3s/step - loss: 68.1242 - mean_squared_error: 252.3777 - mean_absolute_error: 2.9092 - val_loss: 67.6000 - val_mean_squared_error: 147.6348 - val_mean_absolute_error: 2.9092\nEpoch 3/3\n50/50 [==============================] - 169s 3s/step - loss: 68.0721 - mean_squared_error: 204.1793 - mean_absolute_error: 2.8520 - val_loss: 67.6000 - val_mean_squared_error: 147.6348 - val_mean_absolute_error: 2.9092\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00021-9992fc16-a622-4e6a-a988-2ab041be9192",
        "deepnote_cell_type": "code"
      },
      "source": "# Save the entire model as a SavedModel.\n!mkdir -p saved_model\nmodel.save('./saved_model/CNN_model')",
      "execution_count": 47,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "INFO:tensorflow:Assets written to: ./saved_model/CNN_model/assets\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00022-0d8461dc-98b6-4d32-8be0-874feec75f21",
        "deepnote_cell_type": "code"
      },
      "source": "loss, mean_squared_error, mean_ab = model.evaluate(x_val_pad, y_val)",
      "execution_count": 43,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "313/313 [==============================] - 25s 79ms/step - loss: 67.6000 - mean_squared_error: 147.6348 - mean_absolute_error: 2.9092\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00023-f36d5209-f7ca-4885-8983-9b7356de852a",
        "deepnote_cell_type": "code"
      },
      "source": "# prediction = model.predict(x_test_pad)",
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00024-13f9aa60-557f-4817-886b-ceef627134f0",
        "deepnote_cell_type": "code"
      },
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00025-03872566-fecd-40a7-92ad-be4fd1aedb01",
        "deepnote_cell_type": "code"
      },
      "source": "# © Laëtitia CONSTANTIN 2021",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=e80043e2-6875-4b65-a196-a0ffb97a1282' target=\"_blank\">\n<img style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
      "metadata": {
        "tags": [],
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown"
      }
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 4,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "deepnote_notebook_id": "c5fe40e6-aa03-4d3b-a5e7-56b796539873",
    "deepnote": {},
    "deepnote_execution_queue": []
  }
}