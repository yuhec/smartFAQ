{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TP traduction\n",
    "Pour ce tp vous devrez créer votre premier traducteur ! Pour ce faire vous vous appuierez sur une architecture que vous connaisez déjà : Seq2Seq. Pour rappel cette architecture est composée de deux parties qui sont toutes les deux composées de cellules LSTM. \n",
    "- La première partie est appelée Encoder : dans l'intuition ce brique va permettre de transformer le langage source dans un espace latent à plus faible dimension, plus compacte. C'est un petit peu comme si c'était encodé dans un langage que la machine comprenait\n",
    "- LA seconde est le Decoder : son principe est inverse, il va reconvertir l'espace latent vers le langage pour lequl il a été entrainé.\n",
    "    \n",
    "Ce type d'architecture était à l'état de l'art en traduction en 2015, quand vous utilisez google trad à l'époque vous utilisiez déjà un modèle de Deep Learning. \n",
    "\n",
    "Dans ce tp l'exemple sera simplifié pour que vous puissiez l'éxécuter sur votre machine. La principale \"brique\" manquante pour avoir des traducteurs est l'attention. Nous expliquerons ce concept largement dans les prochaines semaines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, GRU, Dense, Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constantes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20\n",
    "LSTM_NODES =256\n",
    "NUM_SENTENCES = 2000 # on commence petit pour la première itération \n",
    "MAX_SENTENCE_LENGTH = 50\n",
    "MAX_NUM_WORDS = 20000\n",
    "EMBEDDING_SIZE = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vous pouvez télécharger les données ici : http://www.manythings.org/anki/fra-eng.zip\n",
    "Ce fichier relativement simple représente différentes traductions Français - Anglais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une fois téléchargé, dézippé les données "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons rajouter deux tokens pour chaque phrase, un pour marquer le début de la phrase et un pour marquer la fin : \"eos\" \"sos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num samples input: 0\n",
      "num samples output: 0\n",
      "num samples output input: 0\n"
     ]
    }
   ],
   "source": [
    "input_sentences = []\n",
    "output_sentences = []\n",
    "output_sentences_inputs = []\n",
    "\n",
    "with open('../../data/x_train.pickle', 'rb') as handle:\n",
    "    X_train = pickle.load(handle)\n",
    "with open('../../data/y_train.pickle', 'rb') as handle:\n",
    "    y_train = pickle.load(handle)\n",
    "with open('../../data/x_val.pickle', 'rb') as handle:\n",
    "    X_val = pickle.load(handle)\n",
    "with open('../../data/y_val.pickle', 'rb') as handle:\n",
    "    y_val = pickle.load(handle)\n",
    "    \n",
    "count = 0\n",
    "for line in X_train['answer']:\n",
    "    count += 1\n",
    "\n",
    "    if count > NUM_SENTENCES:\n",
    "        break\n",
    "\n",
    "    if '\\t' not in line:\n",
    "        continue\n",
    "\n",
    "    input_sentence, output, _ = line.rstrip().split('\\t')\n",
    "\n",
    "    output_sentence = output + ' <eos>'\n",
    "    output_sentence_input = '<sos> ' + output\n",
    "\n",
    "    input_sentences.append(input_sentence)\n",
    "    output_sentences.append(output_sentence)\n",
    "    output_sentences_inputs.append(output_sentence_input)\n",
    "\n",
    "print(\"num samples input:\", len(input_sentences))\n",
    "print(\"num samples output:\", len(output_sentences))\n",
    "print(\"num samples output input:\", len(output_sentences_inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization et Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On tokenize le texte, avec un tokenizer francais et un anglais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Go.',\n",
       " 'Go.',\n",
       " 'Go.',\n",
       " 'Hi.',\n",
       " 'Hi.',\n",
       " 'Run!',\n",
       " 'Run!',\n",
       " 'Run!',\n",
       " 'Run!',\n",
       " 'Run!',\n",
       " 'Run!',\n",
       " 'Run!',\n",
       " 'Run!',\n",
       " 'Run.',\n",
       " 'Run.',\n",
       " 'Run.',\n",
       " 'Run.',\n",
       " 'Run.',\n",
       " 'Run.',\n",
       " 'Run.',\n",
       " 'Run.',\n",
       " 'Who?',\n",
       " 'Wow!',\n",
       " 'Fire!',\n",
       " 'Help!',\n",
       " 'Jump!',\n",
       " 'Jump.',\n",
       " 'Stop!',\n",
       " 'Stop!',\n",
       " 'Stop!',\n",
       " 'Wait!',\n",
       " 'Wait!',\n",
       " 'Wait!',\n",
       " 'Wait.',\n",
       " 'Wait.',\n",
       " 'Wait.',\n",
       " 'Wait.',\n",
       " 'Begin.',\n",
       " 'Begin.',\n",
       " 'Go on.',\n",
       " 'Go on.',\n",
       " 'Go on.',\n",
       " 'Hello!',\n",
       " 'Hello!',\n",
       " 'I see.',\n",
       " 'I see.',\n",
       " 'I try.',\n",
       " 'I won!',\n",
       " 'I won!',\n",
       " 'I won.',\n",
       " 'Oh no!',\n",
       " 'Relax.',\n",
       " 'Relax.',\n",
       " 'Relax.',\n",
       " 'Relax.',\n",
       " 'Relax.',\n",
       " 'Relax.',\n",
       " 'Relax.',\n",
       " 'Relax.',\n",
       " 'Relax.',\n",
       " 'Relax.',\n",
       " 'Relax.',\n",
       " 'Relax.',\n",
       " 'Smile.',\n",
       " 'Smile.',\n",
       " 'Smile.',\n",
       " 'Attack!',\n",
       " 'Attack!',\n",
       " 'Attack!',\n",
       " 'Cheers!',\n",
       " 'Cheers!',\n",
       " 'Cheers!',\n",
       " 'Cheers!',\n",
       " 'Eat it.',\n",
       " 'Eat it.',\n",
       " 'Get up.',\n",
       " 'Get up.',\n",
       " 'Go now.',\n",
       " 'Go now.',\n",
       " 'Go now.',\n",
       " 'Got it!',\n",
       " 'Got it!',\n",
       " 'Got it!',\n",
       " 'Got it?',\n",
       " 'Got it?',\n",
       " 'Got it?',\n",
       " 'Hop in.',\n",
       " 'Hop in.',\n",
       " 'Hug me.',\n",
       " 'Hug me.',\n",
       " 'I fell.',\n",
       " 'I fell.',\n",
       " 'I fled.',\n",
       " 'I knit.',\n",
       " 'I know.',\n",
       " 'I left.',\n",
       " 'I left.',\n",
       " 'I lied.',\n",
       " 'I lost.',\n",
       " 'I paid.',\n",
       " 'I paid.',\n",
       " 'I paid.',\n",
       " 'I quit.',\n",
       " \"I'm 19.\",\n",
       " \"I'm OK.\",\n",
       " \"I'm OK.\",\n",
       " 'Listen.',\n",
       " 'No way!',\n",
       " 'No way!',\n",
       " 'No way!',\n",
       " 'No way!',\n",
       " 'No way!',\n",
       " 'No way!',\n",
       " 'No way!',\n",
       " 'No way!',\n",
       " 'No way!',\n",
       " 'No way!',\n",
       " 'Really?',\n",
       " 'Really?',\n",
       " 'Really?',\n",
       " 'Really?',\n",
       " 'Really?',\n",
       " 'Thanks.',\n",
       " 'Thanks.',\n",
       " 'Try it.',\n",
       " 'Try it.',\n",
       " 'Try it.',\n",
       " 'We try.',\n",
       " 'We won.',\n",
       " 'We won.',\n",
       " 'We won.',\n",
       " 'We won.',\n",
       " 'Ask Tom.',\n",
       " 'Ask him.',\n",
       " 'Awesome!',\n",
       " 'Awesome!',\n",
       " 'Be calm.',\n",
       " 'Be calm.',\n",
       " 'Be calm.',\n",
       " 'Be cool.',\n",
       " 'Be fair.',\n",
       " 'Be fair.',\n",
       " 'Be fair.',\n",
       " 'Be fair.',\n",
       " 'Be fair.',\n",
       " 'Be fair.',\n",
       " 'Be fair.',\n",
       " 'Be fair.',\n",
       " 'Be fair.',\n",
       " 'Be kind.',\n",
       " 'Be nice.',\n",
       " 'Be nice.',\n",
       " 'Be nice.',\n",
       " 'Be nice.',\n",
       " 'Be nice.',\n",
       " 'Be nice.',\n",
       " 'Beat it.',\n",
       " 'Beat it.',\n",
       " 'Beat it.',\n",
       " 'Beat it.',\n",
       " 'Beat it.',\n",
       " 'Beat it.',\n",
       " 'Beat it.',\n",
       " 'Beat it.',\n",
       " 'Beat it.',\n",
       " 'Beat it.',\n",
       " 'Beat it.',\n",
       " 'Beat it.',\n",
       " 'Beat it.',\n",
       " 'Beat it.',\n",
       " 'Beat it.',\n",
       " 'Beat it.',\n",
       " 'Beat it.',\n",
       " 'Beat it.',\n",
       " 'Burn it.',\n",
       " 'Burn it.',\n",
       " 'Burn it.',\n",
       " 'Burn it.',\n",
       " 'Bury it.',\n",
       " 'Bury it.',\n",
       " 'Bury it.',\n",
       " 'Bury it.',\n",
       " 'Call me.',\n",
       " 'Call me.',\n",
       " 'Call us.',\n",
       " 'Call us.',\n",
       " 'Come in.',\n",
       " 'Come in.',\n",
       " 'Come in.',\n",
       " 'Come in.',\n",
       " 'Come on!',\n",
       " 'Come on!',\n",
       " 'Come on!',\n",
       " 'Come on!',\n",
       " 'Come on!',\n",
       " 'Come on!',\n",
       " 'Come on!',\n",
       " 'Come on.',\n",
       " 'Come on.',\n",
       " 'Come on.',\n",
       " 'Drop it!',\n",
       " 'Drop it!',\n",
       " 'Drop it!',\n",
       " 'Drop it!',\n",
       " 'Get Tom.',\n",
       " 'Get out!',\n",
       " 'Get out!',\n",
       " 'Get out!',\n",
       " 'Get out!',\n",
       " 'Get out!',\n",
       " 'Get out!',\n",
       " 'Get out!',\n",
       " 'Get out!',\n",
       " 'Get out!',\n",
       " 'Get out!',\n",
       " 'Get out!',\n",
       " 'Get out!',\n",
       " 'Get out!',\n",
       " 'Get out!',\n",
       " 'Get out!',\n",
       " 'Get out!',\n",
       " 'Get out!',\n",
       " 'Get out.',\n",
       " 'Get out.',\n",
       " 'Get out.',\n",
       " 'Get out.',\n",
       " 'Get out.',\n",
       " 'Get out.',\n",
       " 'Get out.',\n",
       " 'Get out.',\n",
       " 'Get out.',\n",
       " 'Get out.',\n",
       " 'Get out.',\n",
       " 'Get out.',\n",
       " 'Get out.',\n",
       " 'Get out.',\n",
       " 'Get out.',\n",
       " 'Go away!',\n",
       " 'Go away!',\n",
       " 'Go away!',\n",
       " 'Go away!',\n",
       " 'Go away!',\n",
       " 'Go away!',\n",
       " 'Go away!',\n",
       " 'Go away.',\n",
       " 'Go away.',\n",
       " 'Go away.',\n",
       " 'Go away.',\n",
       " 'Go away.',\n",
       " 'Go away.',\n",
       " 'Go away.',\n",
       " 'Go away.',\n",
       " 'Go away.',\n",
       " 'Go away.',\n",
       " 'Go away.',\n",
       " 'Go away.',\n",
       " 'Go away.',\n",
       " 'Go back.',\n",
       " 'Go home.',\n",
       " 'Go home.',\n",
       " 'Go home.',\n",
       " 'Go home.',\n",
       " 'Go slow.',\n",
       " 'Go slow.',\n",
       " 'Goodbye!',\n",
       " 'Goodbye!',\n",
       " 'Goodbye!',\n",
       " 'Hang on!',\n",
       " 'Hang on!',\n",
       " 'Hang on!',\n",
       " 'Hang on.',\n",
       " 'Hang on.',\n",
       " 'Hang on.',\n",
       " 'He left.',\n",
       " 'He quit.',\n",
       " 'He quit.',\n",
       " 'He runs.',\n",
       " 'Help me!',\n",
       " 'Help me.',\n",
       " 'Help me.',\n",
       " 'Help me.',\n",
       " 'Help me.',\n",
       " 'Help us.',\n",
       " 'Help us.',\n",
       " 'Hold it!',\n",
       " 'Hold it!',\n",
       " 'Hold it!',\n",
       " 'Hold it!',\n",
       " 'Hold on.',\n",
       " 'Hold on.',\n",
       " 'Hug Tom.',\n",
       " 'I agree.',\n",
       " 'I cried.',\n",
       " 'I dozed.',\n",
       " 'I dozed.',\n",
       " 'I drive.',\n",
       " 'I drove.',\n",
       " 'I fired.',\n",
       " 'I froze.',\n",
       " 'I froze.',\n",
       " 'I smoke.',\n",
       " 'I snore.',\n",
       " 'I stink.',\n",
       " 'I stood.',\n",
       " 'I stood.',\n",
       " 'I swore.',\n",
       " 'I swore.',\n",
       " 'I tried.',\n",
       " 'I tried.',\n",
       " 'I tried.',\n",
       " 'I waved.',\n",
       " \"I'll go.\",\n",
       " \"I'm Tom.\",\n",
       " \"I'm fat.\",\n",
       " \"I'm fat.\",\n",
       " \"I'm fit.\",\n",
       " \"I'm hit!\",\n",
       " \"I'm hit!\",\n",
       " \"I'm ill.\",\n",
       " \"I'm sad.\",\n",
       " \"I'm sad.\",\n",
       " \"I'm sad.\",\n",
       " \"I'm shy.\",\n",
       " \"I'm wet.\",\n",
       " \"I'm wet.\",\n",
       " \"It's me!\",\n",
       " 'Join us.',\n",
       " 'Join us.',\n",
       " 'Keep it.',\n",
       " 'Keep it.',\n",
       " 'Kick it.',\n",
       " 'Kick it.',\n",
       " 'Kill it.',\n",
       " 'Kill it.',\n",
       " 'Kill it.',\n",
       " 'Kill it.',\n",
       " 'Kiss me.',\n",
       " 'Kiss me.',\n",
       " 'Lie low.',\n",
       " 'Lie low.',\n",
       " 'Me, too.',\n",
       " 'Move on.',\n",
       " 'Move on.',\n",
       " 'Open it.',\n",
       " 'Open it.',\n",
       " 'Open it.',\n",
       " 'Open it.',\n",
       " 'Open up.',\n",
       " 'Open up.',\n",
       " 'Perfect!',\n",
       " 'Pull it.',\n",
       " 'Pull it.',\n",
       " 'Push it.',\n",
       " 'Push it.',\n",
       " 'Push it.',\n",
       " 'Push it.',\n",
       " 'Push it.',\n",
       " 'See you!',\n",
       " 'See you!',\n",
       " 'See you!',\n",
       " 'See you!',\n",
       " 'See you.',\n",
       " 'See you.',\n",
       " 'Show me.',\n",
       " 'Show me.',\n",
       " 'Shut up!',\n",
       " 'Shut up!',\n",
       " 'Shut up!',\n",
       " 'Shut up!',\n",
       " 'Shut up!',\n",
       " 'Sign up.',\n",
       " 'Sign up.',\n",
       " 'Skip it.',\n",
       " 'So long.',\n",
       " 'Take it.',\n",
       " 'Take it.',\n",
       " 'Take it.',\n",
       " 'Take it.',\n",
       " 'Take it.',\n",
       " 'Tell me.',\n",
       " 'Tell me.',\n",
       " 'Tom won.',\n",
       " 'Wake up!',\n",
       " 'Wake up!',\n",
       " 'Wake up!',\n",
       " 'Wake up.',\n",
       " 'Wake up.',\n",
       " 'Wash up.',\n",
       " 'Wash up.',\n",
       " 'We know.',\n",
       " 'We lost.',\n",
       " 'We lost.',\n",
       " 'We lost.',\n",
       " 'We lost.',\n",
       " 'We lost.',\n",
       " 'We lost.',\n",
       " 'We lost.',\n",
       " 'We lost.',\n",
       " 'We lost.',\n",
       " 'We lost.',\n",
       " 'Welcome.',\n",
       " 'Welcome.',\n",
       " 'Who ran?',\n",
       " 'Who won?',\n",
       " 'Who won?',\n",
       " 'You run.',\n",
       " 'You win.',\n",
       " 'Aim high.',\n",
       " 'Aim high.',\n",
       " 'Am I fat?',\n",
       " 'Am I fat?',\n",
       " 'Ask them.',\n",
       " 'Ask them.',\n",
       " 'Back off!',\n",
       " 'Back off!',\n",
       " 'Back off!',\n",
       " 'Back off.',\n",
       " 'Back off.',\n",
       " 'Back off.',\n",
       " 'Back off.',\n",
       " 'Back off.',\n",
       " 'Be a man.',\n",
       " 'Be a man.',\n",
       " 'Be brave.',\n",
       " 'Be brief.',\n",
       " 'Be brief.',\n",
       " 'Be brief.',\n",
       " 'Be brief.',\n",
       " 'Be brief.',\n",
       " 'Be brief.',\n",
       " 'Be still.',\n",
       " 'Be still.',\n",
       " 'Be still.',\n",
       " 'Beats me.',\n",
       " 'Beats me.',\n",
       " 'Call Tom.',\n",
       " 'Call Tom.',\n",
       " 'Cheer up!',\n",
       " 'Cool off!',\n",
       " 'Cuff him.',\n",
       " 'Drive on.',\n",
       " 'Drive on.',\n",
       " 'Drive on.',\n",
       " 'Drive on.',\n",
       " 'Find Tom.',\n",
       " 'Find Tom.',\n",
       " 'Fix this.',\n",
       " 'Fix this.',\n",
       " 'Get away!',\n",
       " 'Get away!',\n",
       " 'Get away!',\n",
       " 'Get away!',\n",
       " 'Get away!',\n",
       " 'Get away!',\n",
       " 'Get away!',\n",
       " 'Get away!',\n",
       " 'Get away!',\n",
       " 'Get away!',\n",
       " 'Get away!',\n",
       " 'Get away!',\n",
       " 'Get away!',\n",
       " 'Get away!',\n",
       " 'Get away!',\n",
       " 'Get away!',\n",
       " 'Get away!',\n",
       " 'Get away!',\n",
       " 'Get away!',\n",
       " 'Get away!',\n",
       " 'Get away!',\n",
       " 'Get away!',\n",
       " 'Get away!',\n",
       " 'Get away!',\n",
       " 'Get away.',\n",
       " 'Get away.',\n",
       " 'Get down!',\n",
       " 'Get down!',\n",
       " 'Get down!',\n",
       " 'Get down!',\n",
       " 'Get down.',\n",
       " 'Get down.',\n",
       " 'Get down.',\n",
       " 'Get down.',\n",
       " 'Get down.',\n",
       " 'Get down.',\n",
       " 'Get down.',\n",
       " 'Get lost!',\n",
       " 'Get lost!',\n",
       " 'Get lost!',\n",
       " 'Get lost!',\n",
       " 'Get lost!',\n",
       " 'Get lost!',\n",
       " 'Get lost!',\n",
       " 'Get lost!',\n",
       " 'Get lost.',\n",
       " 'Get lost.',\n",
       " 'Get lost.',\n",
       " 'Get lost.',\n",
       " 'Get lost.',\n",
       " 'Get real!',\n",
       " 'Go ahead!',\n",
       " 'Go ahead!',\n",
       " 'Go ahead!',\n",
       " 'Go ahead!',\n",
       " 'Go ahead!',\n",
       " 'Go ahead!',\n",
       " 'Go ahead!',\n",
       " 'Go ahead!',\n",
       " 'Go ahead!',\n",
       " 'Go ahead!',\n",
       " 'Go ahead!',\n",
       " 'Go ahead!',\n",
       " 'Go ahead!',\n",
       " 'Go ahead.',\n",
       " 'Go ahead.',\n",
       " 'Go ahead.',\n",
       " 'Go ahead.',\n",
       " 'Go ahead.',\n",
       " 'Go ahead.',\n",
       " 'Go ahead.',\n",
       " 'Go ahead.',\n",
       " 'Go ahead.',\n",
       " 'Go ahead.',\n",
       " 'Go ahead.',\n",
       " 'Go ahead.',\n",
       " 'Good job!',\n",
       " 'Good job!',\n",
       " 'Good job!',\n",
       " 'Good job!',\n",
       " 'Grab him.',\n",
       " 'Grab him.',\n",
       " 'Have fun.',\n",
       " 'Have fun.',\n",
       " 'He spoke.',\n",
       " 'He spoke.',\n",
       " 'He spoke.',\n",
       " 'He tries.',\n",
       " \"He's wet.\",\n",
       " 'Help Tom.',\n",
       " 'Help Tom.',\n",
       " 'Help Tom.',\n",
       " 'Hi, guys.',\n",
       " 'How cute!',\n",
       " 'How cute!',\n",
       " 'How cute!',\n",
       " 'How deep?',\n",
       " 'How nice!',\n",
       " 'How nice!',\n",
       " 'How nice!',\n",
       " 'How nice!',\n",
       " 'How rude!',\n",
       " 'How wise!',\n",
       " 'Hurry up.',\n",
       " 'Hurry up.',\n",
       " 'Hurry up.',\n",
       " 'Hurry up.',\n",
       " 'Hurry up.',\n",
       " 'Hurry up.',\n",
       " 'I cursed.',\n",
       " 'I did OK.',\n",
       " 'I did OK.',\n",
       " 'I did it.',\n",
       " 'I did it.',\n",
       " 'I failed.',\n",
       " 'I forgot.',\n",
       " 'I get it.',\n",
       " 'I got it.',\n",
       " 'I got it.',\n",
       " 'I helped.',\n",
       " 'I jumped.',\n",
       " 'I looked.',\n",
       " 'I moaned.',\n",
       " 'I nodded.',\n",
       " 'I obeyed.',\n",
       " 'I phoned.',\n",
       " 'I phoned.',\n",
       " 'I refuse.',\n",
       " 'I refuse.',\n",
       " 'I rested.',\n",
       " 'I rested.',\n",
       " 'I saw it.',\n",
       " 'I saw it.',\n",
       " 'I sighed.',\n",
       " 'I smiled.',\n",
       " 'I stayed.',\n",
       " 'I stayed.',\n",
       " 'I talked.',\n",
       " 'I use it.',\n",
       " 'I use it.',\n",
       " 'I use it.',\n",
       " \"I'll pay.\",\n",
       " \"I'll pay.\",\n",
       " \"I'll try.\",\n",
       " \"I'll try.\",\n",
       " \"I'm back.\",\n",
       " \"I'm back.\",\n",
       " \"I'm bald.\",\n",
       " \"I'm busy.\",\n",
       " \"I'm busy.\",\n",
       " \"I'm calm.\",\n",
       " \"I'm cold.\",\n",
       " \"I'm cool.\",\n",
       " \"I'm cool.\",\n",
       " \"I'm deaf.\",\n",
       " \"I'm deaf.\",\n",
       " \"I'm done.\",\n",
       " \"I'm fair.\",\n",
       " \"I'm fair.\",\n",
       " \"I'm fair.\",\n",
       " \"I'm fast.\",\n",
       " \"I'm fine.\",\n",
       " \"I'm fine.\",\n",
       " \"I'm fine.\",\n",
       " \"I'm free!\",\n",
       " \"I'm free.\",\n",
       " \"I'm free.\",\n",
       " \"I'm full.\",\n",
       " \"I'm full.\",\n",
       " \"I'm game.\",\n",
       " \"I'm game.\",\n",
       " \"I'm glad.\",\n",
       " \"I'm home.\",\n",
       " \"I'm late.\",\n",
       " \"I'm lazy.\",\n",
       " \"I'm lazy.\",\n",
       " \"I'm lazy.\",\n",
       " \"I'm lazy.\",\n",
       " \"I'm okay.\",\n",
       " \"I'm okay.\",\n",
       " \"I'm rich.\",\n",
       " \"I'm safe.\",\n",
       " \"I'm sick.\",\n",
       " \"I'm sure.\",\n",
       " \"I'm sure.\",\n",
       " \"I'm sure.\",\n",
       " \"I'm sure.\",\n",
       " \"I'm tall.\",\n",
       " \"I'm thin.\",\n",
       " \"I'm tidy.\",\n",
       " \"I'm tidy.\",\n",
       " \"I'm ugly.\",\n",
       " \"I'm ugly.\",\n",
       " \"I'm weak.\",\n",
       " \"I'm well.\",\n",
       " \"I'm well.\",\n",
       " \"I've won.\",\n",
       " \"I've won.\",\n",
       " 'It helps.',\n",
       " 'It hurts.',\n",
       " 'It works.',\n",
       " 'It works.',\n",
       " \"It's Tom.\",\n",
       " \"It's fun.\",\n",
       " \"It's fun.\",\n",
       " \"It's his.\",\n",
       " \"It's his.\",\n",
       " \"It's new.\",\n",
       " \"It's new.\",\n",
       " \"It's odd.\",\n",
       " \"It's red.\",\n",
       " \"It's sad.\",\n",
       " 'Keep out!',\n",
       " 'Keep out.',\n",
       " 'Kill Tom.',\n",
       " 'Kill Tom.',\n",
       " 'Kiss Tom.',\n",
       " 'Leave it.',\n",
       " 'Leave it.',\n",
       " 'Leave it.',\n",
       " 'Leave it.',\n",
       " 'Leave it.',\n",
       " 'Leave me.',\n",
       " 'Leave us.',\n",
       " 'Leave us.',\n",
       " \"Let's go!\",\n",
       " \"Let's go.\",\n",
       " 'Look out!',\n",
       " 'Look out!',\n",
       " 'Look out!',\n",
       " 'Look out!',\n",
       " 'Look out!',\n",
       " 'Look out!',\n",
       " 'Look out!',\n",
       " 'Look out!',\n",
       " 'Look out!',\n",
       " 'Look out!',\n",
       " 'Marry me.',\n",
       " 'Marry me.',\n",
       " 'May I go?',\n",
       " 'May I go?',\n",
       " 'May I go?',\n",
       " 'Run away.',\n",
       " 'Run away.',\n",
       " 'Run away.',\n",
       " 'Run away.',\n",
       " 'Save Tom.',\n",
       " 'Save Tom.',\n",
       " 'Say what?',\n",
       " 'She came.',\n",
       " 'She died.',\n",
       " 'She left.',\n",
       " 'She runs.',\n",
       " 'Sit down!',\n",
       " 'Sit down!',\n",
       " 'Sit down!',\n",
       " 'Sit down.',\n",
       " 'Sit down.',\n",
       " 'Sit here.',\n",
       " 'Sit here.',\n",
       " 'Speak up!',\n",
       " 'Speak up!',\n",
       " 'Stand up.',\n",
       " 'Stop Tom.',\n",
       " 'Stop Tom.',\n",
       " 'Take Tom.',\n",
       " 'Taste it.',\n",
       " 'Taste it.',\n",
       " 'Taste it.',\n",
       " 'Taste it.',\n",
       " 'Tell Tom.',\n",
       " 'Tell Tom.',\n",
       " 'Terrific!',\n",
       " 'Terrific!',\n",
       " 'Terrific!',\n",
       " 'Terrific!',\n",
       " 'Terrific!',\n",
       " 'Terrific!',\n",
       " 'Terrific!',\n",
       " 'Terrific!',\n",
       " 'Terrific!',\n",
       " 'Terrific!',\n",
       " 'Terrific!',\n",
       " 'Terrific!',\n",
       " 'Terrific!',\n",
       " 'Terrific!',\n",
       " 'Terrific!',\n",
       " 'Terrific!',\n",
       " 'Terrific!',\n",
       " 'They won.',\n",
       " 'They won.',\n",
       " 'They won.',\n",
       " 'They won.',\n",
       " 'Tom came.',\n",
       " 'Tom died.',\n",
       " 'Tom knew.',\n",
       " 'Tom left.',\n",
       " 'Tom left.',\n",
       " 'Tom lied.',\n",
       " 'Tom lies.',\n",
       " 'Tom lost.',\n",
       " 'Tom paid.',\n",
       " 'Tom paid.',\n",
       " 'Tom went.',\n",
       " \"Tom's up.\",\n",
       " 'Too late.',\n",
       " 'Touch it.',\n",
       " 'Touch it.',\n",
       " 'Touch it.',\n",
       " 'Touch it.',\n",
       " 'Trust me.',\n",
       " 'Trust me.',\n",
       " 'Try some.',\n",
       " 'Try some.',\n",
       " 'Try some.',\n",
       " 'Try this.',\n",
       " 'Try this.',\n",
       " 'Try this.',\n",
       " 'Use this.',\n",
       " 'Use this.',\n",
       " 'Use this.',\n",
       " 'Use this.',\n",
       " 'Warn Tom.',\n",
       " 'Warn Tom.',\n",
       " 'Watch me.',\n",
       " 'Watch me.',\n",
       " 'Watch us.',\n",
       " 'Watch us.',\n",
       " 'We agree.',\n",
       " \"We'll go.\",\n",
       " \"We're OK.\",\n",
       " 'What for?',\n",
       " 'What for?',\n",
       " 'What fun!',\n",
       " 'What fun!',\n",
       " 'Who came?',\n",
       " 'Who died?',\n",
       " 'Who fell?',\n",
       " 'Who lost?',\n",
       " 'Who quit?',\n",
       " \"Who's he?\",\n",
       " 'Write me.',\n",
       " 'Write me.',\n",
       " 'You lost.',\n",
       " 'You lost.',\n",
       " 'After you.',\n",
       " 'After you.',\n",
       " 'Aim. Fire!',\n",
       " 'Am I late?',\n",
       " 'Answer me.',\n",
       " 'Be honest.',\n",
       " 'Be honest.',\n",
       " 'Be honest.',\n",
       " 'Be seated.',\n",
       " 'Be seated.',\n",
       " 'Be seated.',\n",
       " 'Birds fly.',\n",
       " 'Bless you.',\n",
       " 'Call home!',\n",
       " 'Calm down!',\n",
       " 'Calm down!',\n",
       " 'Calm down!',\n",
       " 'Calm down.',\n",
       " 'Calm down.',\n",
       " 'Calm down.',\n",
       " 'Can we go?',\n",
       " 'Can we go?',\n",
       " 'Can we go?',\n",
       " 'Catch Tom.',\n",
       " 'Catch Tom.',\n",
       " 'Catch him.',\n",
       " 'Catch him.',\n",
       " 'Chill out.',\n",
       " 'Chill out.',\n",
       " 'Choose me.',\n",
       " 'Come back.',\n",
       " 'Come back.',\n",
       " 'Come here.',\n",
       " 'Come here.',\n",
       " 'Come over!',\n",
       " 'Come over!',\n",
       " 'Come over.',\n",
       " 'Come over.',\n",
       " 'Come over.',\n",
       " 'Come over.',\n",
       " 'Come over.',\n",
       " 'Come soon.',\n",
       " 'Come soon.',\n",
       " 'Cool down.',\n",
       " 'Did I win?',\n",
       " 'Did I win?',\n",
       " 'Did I win?',\n",
       " 'Do it now.',\n",
       " 'Dogs bark.',\n",
       " 'Dogs bark.',\n",
       " \"Don't ask.\",\n",
       " \"Don't cry.\",\n",
       " \"Don't die.\",\n",
       " \"Don't die.\",\n",
       " \"Don't lie.\",\n",
       " \"Don't lie.\",\n",
       " \"Don't run.\",\n",
       " \"Don't run.\",\n",
       " 'Excuse me.',\n",
       " 'Excuse me.',\n",
       " 'Excuse me?',\n",
       " 'Excuse me?',\n",
       " 'Excuse me?',\n",
       " 'Excuse me?',\n",
       " 'Excuse me?',\n",
       " 'Fantastic!',\n",
       " 'Fantastic!',\n",
       " 'Feel this.',\n",
       " 'Feel this.',\n",
       " 'Feel this.',\n",
       " 'Feel this.',\n",
       " 'Film this.',\n",
       " 'Film this.',\n",
       " 'Follow me.',\n",
       " 'Follow us.',\n",
       " 'Follow us.',\n",
       " 'Forget it!',\n",
       " 'Forget it!',\n",
       " 'Forget it!',\n",
       " 'Forget it!',\n",
       " 'Forget it!',\n",
       " 'Forget it!',\n",
       " 'Forget it.',\n",
       " 'Forget it.',\n",
       " 'Forget it.',\n",
       " 'Forget it.',\n",
       " 'Forget it.',\n",
       " 'Forget me.',\n",
       " 'Forget me.',\n",
       " 'Get a job.',\n",
       " 'Get a job.',\n",
       " 'Get a job.',\n",
       " 'Get a job.',\n",
       " 'Get going.',\n",
       " 'Get going.',\n",
       " 'Get going.',\n",
       " 'Get going.',\n",
       " 'Get going.',\n",
       " 'Get going.',\n",
       " 'Get going.',\n",
       " 'Get going.',\n",
       " 'Get ready.',\n",
       " 'Get ready.',\n",
       " 'Go get it.',\n",
       " 'Go get it.',\n",
       " 'Go inside.',\n",
       " 'Go to bed.',\n",
       " 'Go to bed.',\n",
       " 'Go to bed.',\n",
       " 'Go to bed.',\n",
       " 'Go to bed.',\n",
       " 'Go to bed.',\n",
       " 'Go to bed.',\n",
       " 'Good luck.',\n",
       " 'Good luck.',\n",
       " 'Good luck.',\n",
       " 'Grab that.',\n",
       " 'Grab that.',\n",
       " 'Grab that.',\n",
       " 'Grab that.',\n",
       " 'Grab this.',\n",
       " 'Grab this.',\n",
       " 'Hands off.',\n",
       " 'Have some.',\n",
       " 'Have some.',\n",
       " 'He is ill.',\n",
       " 'He is old.',\n",
       " \"He's a DJ.\",\n",
       " \"He's good.\",\n",
       " \"He's lazy.\",\n",
       " \"He's mine.\",\n",
       " \"He's rich.\",\n",
       " \"He's sexy.\",\n",
       " 'Head east.',\n",
       " 'Head east.',\n",
       " 'Head west.',\n",
       " 'Head west.',\n",
       " 'Here I am.',\n",
       " \"Here's $5.\",\n",
       " 'Hold fire.',\n",
       " 'Hold fire.',\n",
       " 'Hold this.',\n",
       " 'Hold this.',\n",
       " 'Hold this.',\n",
       " 'Hold this.',\n",
       " 'How awful!',\n",
       " 'How awful!',\n",
       " 'How weird!',\n",
       " \"How's Tom?\",\n",
       " \"How's Tom?\",\n",
       " 'Humor Tom.',\n",
       " 'Humor Tom.',\n",
       " 'I am cold.',\n",
       " 'I am good.',\n",
       " 'I am lost.',\n",
       " 'I am lost.',\n",
       " 'I am okay.',\n",
       " 'I am sick.',\n",
       " 'I am sure.',\n",
       " 'I am sure.',\n",
       " 'I beg you.',\n",
       " 'I beg you.',\n",
       " 'I beg you.',\n",
       " 'I beg you.',\n",
       " 'I can fly.',\n",
       " 'I can run.',\n",
       " 'I can ski.',\n",
       " 'I cheered.',\n",
       " 'I cringed.',\n",
       " 'I cringed.',\n",
       " 'I cringed.',\n",
       " 'I exhaled.',\n",
       " 'I gave up.',\n",
       " 'I give in.',\n",
       " 'I give up.',\n",
       " 'I got hot.',\n",
       " 'I got hot.',\n",
       " 'I grinned.',\n",
       " 'I had fun.',\n",
       " 'I had fun.',\n",
       " 'I had fun.',\n",
       " 'I had fun.',\n",
       " 'I hate it.',\n",
       " 'I have it.',\n",
       " 'I hit Tom.',\n",
       " 'I hope so.',\n",
       " 'I hurried.',\n",
       " 'I hurried.',\n",
       " 'I inhaled.',\n",
       " 'I knew it.',\n",
       " 'I like it.',\n",
       " 'I like it.',\n",
       " 'I like it.',\n",
       " 'I lost it.',\n",
       " 'I love it!',\n",
       " 'I love it.',\n",
       " 'I mean it!',\n",
       " 'I mean it.',\n",
       " 'I must go.',\n",
       " 'I must go.',\n",
       " 'I must go.',\n",
       " 'I must go.',\n",
       " 'I must go.',\n",
       " 'I must go.',\n",
       " 'I must go.',\n",
       " 'I must go.',\n",
       " 'I need it.',\n",
       " ...]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique words in the input: 552\n",
      "Length of longest sentence in input: 4\n"
     ]
    }
   ],
   "source": [
    "input_tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "input_tokenizer.fit_on_texts(input_sentences)\n",
    "input_integer_seq = input_tokenizer.texts_to_sequences(input_sentences)\n",
    "\n",
    "word2idx_inputs = input_tokenizer.word_index\n",
    "print('Total unique words in the input: %s' % len(word2idx_inputs))\n",
    "\n",
    "max_input_len = max(len(sen) for sen in input_integer_seq)\n",
    "print(\"Length of longest sentence in input: %g\" % max_input_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique words in the output: 1508\n",
      "Length of longest sentence in the output: 11\n"
     ]
    }
   ],
   "source": [
    "output_tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, filters='')\n",
    "output_tokenizer.fit_on_texts(output_sentences + output_sentences_inputs)\n",
    "output_integer_seq = output_tokenizer.texts_to_sequences(output_sentences)\n",
    "output_input_integer_seq = output_tokenizer.texts_to_sequences(output_sentences_inputs)\n",
    "\n",
    "word2idx_outputs = output_tokenizer.word_index\n",
    "print('Total unique words in the output: %s' % len(word2idx_outputs))\n",
    "\n",
    "num_words_output = len(word2idx_outputs) + 1\n",
    "max_out_len = max(len(sen) for sen in output_integer_seq)\n",
    "print(\"Length of longest sentence in the output: %g\" % max_out_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On applique maintenat un padding : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_input_sequences.shape: (2000, 4)\n",
      "encoder_input_sequences[172]: [  0   0   0 173]\n"
     ]
    }
   ],
   "source": [
    "encoder_input_sequences = pad_sequences(input_integer_seq, maxlen=max_input_len)\n",
    "print(\"encoder_input_sequences.shape:\", encoder_input_sequences.shape)\n",
    "print(\"encoder_input_sequences[172]:\", encoder_input_sequences[72])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_input_sequences.shape: (2000, 11)\n",
      "decoder_input_sequences[172]: [  2 602   3   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "decoder_input_sequences = pad_sequences(output_input_integer_seq, maxlen=max_out_len, padding='post')\n",
    "print(\"decoder_input_sequences.shape:\", decoder_input_sequences.shape)\n",
    "print(\"decoder_input_sequences[172]:\", decoder_input_sequences[72])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "print(word2idx_outputs[\"<sos>\"])\n",
    "print(word2idx_outputs[\"je\"])\n",
    "print(word2idx_outputs[\"suis\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons vectorisé les mots en utilisant des embeddings déjà entrainé. Pour changer nous allons utiliser ceux de Glove ! Vous pouvez télécharger une version ici : https://www.kaggle.com/danielwillgeorge/glove6b100dtxt\n",
    "        - Cette version contient des vecteurs de taille 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le fichier contient une ligne par mot, pour chaque ligne le premier élément est le mot, la suite de la ligne est constituée d'une liste de 100 valeurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "\n",
    "embeddings_dictionary = dict()\n",
    "\n",
    "glove_file = open(r'../../data/fra-eng/glove.6B.100d.txt', encoding=\"utf8\")\n",
    "\n",
    "\n",
    "#On se crée un dictionnaire pour pouvoir facilement travailler avec les vecteurs de glove, en insérant en cé chaque mot et en valeur la liste des vecteurs\n",
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    #le mot\n",
    "    word = records[0]\n",
    "    vector_dimensions = asarray(records[1:], dtype='float32') # on récupère la liste des valeurs\n",
    "    embeddings_dictionary[word] = vector_dimensions\n",
    "glove_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = min(MAX_NUM_WORDS, len(word2idx_inputs) + 1)\n",
    "# on initilialise une matrice vide que l'on va remplir\n",
    "embedding_matrix = zeros((num_words, EMBEDDING_SIZE))\n",
    "\n",
    "for word, index in word2idx_inputs.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut désormias facilement à partir d'un mot récupérer son vecteur : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.038194 -0.24487   0.72812  -0.39961   0.083172  0.043953 -0.39141\n",
      "  0.3344   -0.57545   0.087459  0.28787  -0.06731   0.30906  -0.26384\n",
      " -0.13231  -0.20757   0.33395  -0.33848  -0.31743  -0.48336   0.1464\n",
      " -0.37304   0.34577   0.052041  0.44946  -0.46971   0.02628  -0.54155\n",
      " -0.15518  -0.14107  -0.039722  0.28277   0.14393   0.23464  -0.31021\n",
      "  0.086173  0.20397   0.52624   0.17164  -0.082378 -0.71787  -0.41531\n",
      "  0.20335  -0.12763   0.41367   0.55187   0.57908  -0.33477  -0.36559\n",
      " -0.54857  -0.062892  0.26584   0.30205   0.99775  -0.80481  -3.0243\n",
      "  0.01254  -0.36942   2.2167    0.72201  -0.24978   0.92136   0.034514\n",
      "  0.46745   1.1079   -0.19358  -0.074575  0.23353  -0.052062 -0.22044\n",
      "  0.057162 -0.15806  -0.30798  -0.41625   0.37972   0.15006  -0.53212\n",
      " -0.2055   -1.2526    0.071624  0.70565   0.49744  -0.42063   0.26148\n",
      " -1.538    -0.30223  -0.073438 -0.28312   0.37104  -0.25217   0.016215\n",
      " -0.017099 -0.38984   0.87424  -0.72569  -0.51058  -0.52028  -0.1459\n",
      "  0.8278    0.27062 ]\n"
     ]
    }
   ],
   "source": [
    "print(embeddings_dictionary[\"the\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il faut maintenant créer une matrice \"y_true\" one_hot de sortie. Cette matrice est de taille (nombre de lignes à prédire, longueur maximale d'une phrase, nombre de mots dans le vocabulaire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "553"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_input_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On crée une matrice vide à l'aide de np.zeros\n",
    "decoder_targets_one_hot = np.zeros((\n",
    "        len(input_sentences),\n",
    "        max_out_len,\n",
    "        num_words_output\n",
    "    ),\n",
    "    dtype='float32'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 11, 1509)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_targets_one_hot.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remplit maintenant cette matrice vide à partir des données que l'on doit prédire contenues dans la variable output_integer_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_output_sequences = pad_sequences(output_integer_seq, maxlen=max_out_len, padding='post')\n",
    "for i, d in enumerate(decoder_output_sequences):\n",
    "    for t, word in enumerate(d):\n",
    "        decoder_targets_one_hot[i, t, word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_targets_one_hot[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder \n",
    "L'encoder va prendre en entrée les phrases anglaises et va générer un espace latent. On va utiliser pour cela la variable LSTM_NODES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n"
     ]
    }
   ],
   "source": [
    "print(LSTM_NODES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans Keras il existe une couche que vous avez déjà utilisé : Embedding, nous allons utiliser cette couche en entrée de notre réseau, suivi d'une couche LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'input_2:0' shape=(None, 4) dtype=float32>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_inputs_placeholder = Input(shape=(max_input_len,))\n",
    "encoder_inputs_placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs_placeholder = Input(shape=(max_input_len,))\n",
    "x = Embedding(num_words, EMBEDDING_SIZE, \n",
    "              weights=[embedding_matrix], \n",
    "              input_length=max_input_len)(encoder_inputs_placeholder)\n",
    "encoder = LSTM(LSTM_NODES, return_state=True)\n",
    "\n",
    "encoder_outputs, h, c = encoder(x)\n",
    "encoder_states = [h, c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N'hésitez pas à vous faire un schéma pour bien comprendre, le réseau décodeur va avoir plusieurs entrées, d'une part l'espace latent fournis par l'encodeur, d'autre part la prédiction de l'étape précédente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#on utilise une fois de plus la couche Embedding pour utiliser la prédiction précédente\n",
    "decoder_inputs_placeholder = Input(shape=(max_out_len,))\n",
    "\n",
    "decoder_embedding = Embedding(num_words_output, LSTM_NODES)\n",
    "decoder_inputs_x = decoder_embedding(decoder_inputs_placeholder)\n",
    "\n",
    "decoder_outputs, _, _ = LSTM(LSTM_NODES, return_sequences=True, return_state=True)(decoder_inputs_x, initial_state=encoder_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully connected final pour générer la prédiction\n",
    "On rajoute une couche dense après le décoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_dense = Dense(num_words_output, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#La syntaxe est un peu différente car notre modèle doit avoir deux entrées, la phrase anglaise pour l'encoder et le token sos pour le decoder\n",
    "model = Model([encoder_inputs_placeholder,\n",
    "  decoder_inputs_placeholder], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.compile(\n",
    "    optimizer='rmsprop',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affichage du modèle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 4)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 11)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 4, 100)       55300       input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 11, 256)      386304      input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 256), (None, 365568      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 11, 256), (N 525312      embedding_1[0][0]                \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 11, 1509)     387813      lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 1,720,297\n",
      "Trainable params: 1,720,297\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\pierre.leroy\\appdata\\local\\continuum\\miniconda3\\envs\\comp4drone\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1800 samples, validate on 200 samples\n",
      "Epoch 1/2\n",
      "1800/1800 [==============================] - 11s 6ms/step - loss: 1.2965 - accuracy: 0.7994 - val_loss: 1.6994 - val_accuracy: 0.7809\n",
      "Epoch 2/2\n",
      "1800/1800 [==============================] - 9s 5ms/step - loss: 1.1214 - accuracy: 0.8125 - val_loss: 1.6436 - val_accuracy: 0.7832\n"
     ]
    }
   ],
   "source": [
    "r = model.fit(\n",
    "    [encoder_input_sequences, decoder_input_sequences],\n",
    "    decoder_targets_one_hot,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_split=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptation du modèle pour la prédiction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour la prédiction finale du modèle nous modifions un peu le réseau pour générer notre prédiction. En effet lors de notre entrainement nous envoyons à chaque fois le mot réel -1 dans le décoder, cela n'est pas possible lors d'une prédiction finale puisque nous ne connaissons pas la traduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l'encoder du modèle ne change pas, on l'encapsule juste dans un objet modèle : \n",
    "\n",
    "#encoder\n",
    "encoder_model = Model(encoder_inputs_placeholder, encoder_states)\n",
    "\n",
    "#decoder\n",
    "##  espace latent\n",
    "decoder_state_input_h = Input(shape=(LSTM_NODES,))\n",
    "decoder_state_input_c = Input(shape=(LSTM_NODES,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "#récupération de mot précédemment prédit\n",
    "decoder_inputs_single = Input(shape=(1,))\n",
    "decoder_inputs_single_x = decoder_embedding(decoder_inputs_single)\n",
    "\n",
    "decoder_outputs, h, c = LSTM(LSTM_NODES, return_sequences=True, return_state=True)(decoder_inputs_single_x, initial_state=decoder_states_inputs)\n",
    "\n",
    "decoder_states = [h, c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "#on encapsule tout le decoder dans un objet modèle\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs_single] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word_input = {v:k for k, v in word2idx_inputs.items()}\n",
    "idx2word_target = {v:k for k, v in word2idx_outputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(input_seq):\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    target_seq[0, 0] = word2idx_outputs['<sos>']\n",
    "    eos = word2idx_outputs['<eos>']\n",
    "    output_sentence = []\n",
    "\n",
    "    for _ in range(max_out_len):\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "        idx = np.argmax(output_tokens[0, 0, :])\n",
    "\n",
    "        if eos == idx:\n",
    "            break\n",
    "\n",
    "        word = ''\n",
    "\n",
    "        if idx > 0:\n",
    "            word = idx2word_target[idx]\n",
    "            output_sentence.append(word)\n",
    "\n",
    "        target_seq[0, 0] = idx\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return ' '.join(output_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.choice(len(input_sentences))\n",
    "input_seq = encoder_input_sequences[i:i+1]\n",
    "translation = translate_sentence(input_seq)\n",
    "print('-')\n",
    "print('Input:', input_sentences[i])\n",
    "print('Response:', translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pour aller plus loin \n",
    "Maintenant que vous avez un premier modèle qui fonctionne vous allez essayer d'augmenter les performances de votre modèle, voici plusieurs pistes :\n",
    "- Lancez tensorboard et relancer l'entrainement en ajoutant un callback pour suivre l'évolution de votre modèle\n",
    "- Calculez un BLEU score \n",
    "- augmenter la valeur de vos paramètres \n",
    "- comparez avec d'autres embeddings comme word2vec, qu'est ce qui fonctionne le mieux ?\n",
    "- Essayez d'utiliser des embeddings plus grands, 200 ? 300 ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source du tp : https://stackabuse.com/python-for-nlp-neural-machine-translation-with-seq2seq-in-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
