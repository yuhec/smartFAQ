{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Embedding, LSTM, Dense, Activation, Conv1D, Flatten\n",
    "\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load w2v model & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../../data/w2v_pretrained_weights.pickle', 'rb') as handle:\n",
    "#     w2v_model = pickle.load(handle)\n",
    "with open('../../data/x_train.pickle', 'rb') as handle:\n",
    "    X_train = pickle.load(handle)\n",
    "with open('../../data/y_train.pickle', 'rb') as handle:\n",
    "    y_train = pickle.load(handle)\n",
    "with open('../../data/x_val.pickle', 'rb') as handle:\n",
    "    X_val = pickle.load(handle)\n",
    "with open('../../data/y_val.pickle', 'rb') as handle:\n",
    "    y_val = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrained_weights = w2v_model.wv.syn0\n",
    "# vocab_size, embedding_size = pretrained_weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization & Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NUM_WORDS = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique words in the input: 30369\n",
      "Length of longest sentence in input: 1688\n"
     ]
    }
   ],
   "source": [
    "input_tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "input_tokenizer.fit_on_texts(X_train['answer'])\n",
    "input_integer_seq = input_tokenizer.texts_to_sequences(X_train['answer'])\n",
    "\n",
    "word2idx_inputs = input_tokenizer.word_index\n",
    "print('Total unique words in the input: %s' % len(word2idx_inputs))\n",
    "\n",
    "max_input_len = max(len(sen) for sen in input_integer_seq)\n",
    "print(\"Length of longest sentence in input: %g\" % max_input_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_input_sequences.shape: (10000, 1688)\n",
      "encoder_input_sequences[172]: [  0   0   0 ... 135 183  86]\n"
     ]
    }
   ],
   "source": [
    "x_train_pad = pad_sequences(input_integer_seq, maxlen=max_input_len)\n",
    "# x_train_pad = pad_sequences(input_integer_seq, maxlen=MAX_SEQ_LEN)\n",
    "print(\"encoder_input_sequences.shape:\", x_train_pad.shape)\n",
    "print(\"encoder_input_sequences[172]:\", x_train_pad[72])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_input_sequences.shape: (10000, 1688)\n",
      "encoder_input_sequences[172]: [   0    0    0 ... 5837 5419 5838]\n"
     ]
    }
   ],
   "source": [
    "input_tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "input_tokenizer.fit_on_texts(X_val['answer'])\n",
    "input_integer_seq = input_tokenizer.texts_to_sequences(X_val['answer'])\n",
    "x_val_pad = pad_sequences(input_integer_seq, maxlen=max_input_len)\n",
    "print(\"encoder_input_sequences.shape:\", x_val_pad.shape)\n",
    "print(\"encoder_input_sequences[172]:\", x_val_pad[72])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "sentences = [[word for word in document.lower().split()] for document in X_train['answer']]\n",
    "\n",
    "word_model = Word2Vec(sentences, size=200, min_count = 1, window = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "pretrained_weights = word_model.wv.syn0\n",
    "vocab_size, embedding_size = pretrained_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size,embedding_size,pretrained_weights):\n",
    "    \n",
    "           \n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    model.add(Embedding(input_dim=vocab_size, \n",
    "                        output_dim=embedding_size, \n",
    "                        weights=[pretrained_weights],\n",
    "                        input_length=max_input_len\n",
    "                       ))\n",
    "\n",
    "    model.add(Conv1D(128,5, activation='relu'))\n",
    "    model.add(Conv1D(128,5, activation='relu'))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "#     model.add(LSTM(units=embedding_size))\n",
    "#     model.add(Dense(units=vocab_size))\n",
    "#     model.add(Activation('softmax'))\n",
    "#     model.add(Dense(1))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model(model):\n",
    "    mse = tf.keras.losses.MeanSquaredError()\n",
    "    model.compile(loss=mse,\n",
    "                  optimizer='adam',\n",
    "                  metrics=['MeanSquaredError']\n",
    "                  )\n",
    "    return  model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model( x_train, y_train, x_val, y_val, model, batch_size,  epochs = 5):\n",
    "    \n",
    "    print('Train...')\n",
    "    os.makedirs(\"./logs\",exist_ok=True)\n",
    "    tensorboard = TensorBoard(log_dir=os.path.join('./logs'), histogram_freq=0,\n",
    "                                  write_graph=True, write_images=False,profile_batch = 100000000)\n",
    "\n",
    "    callbacks = [ tensorboard]\n",
    "\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_val, y_val),\n",
    "              callbacks= callbacks)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 1688, 200)         6074000   \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 1684, 128)         128128    \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 1680, 128)         82048     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 215040)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                2150410   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 8,434,597\n",
      "Trainable params: 8,434,597\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train...\n",
      "Epoch 1/10\n",
      "50/50 [==============================] - 128s 3s/step - loss: 188.2514 - mean_squared_error: 188.2514 - val_loss: 142.9084 - val_mean_squared_error: 142.9084\n",
      "Epoch 2/10\n",
      "50/50 [==============================] - 124s 2s/step - loss: 132.6154 - mean_squared_error: 132.6154 - val_loss: 142.9084 - val_mean_squared_error: 142.9084\n",
      "Epoch 3/10\n",
      "50/50 [==============================] - 125s 3s/step - loss: 126.4466 - mean_squared_error: 126.4466 - val_loss: 142.9084 - val_mean_squared_error: 142.9084\n",
      "Epoch 4/10\n",
      "50/50 [==============================] - 126s 3s/step - loss: 121.7370 - mean_squared_error: 121.7370 - val_loss: 142.9084 - val_mean_squared_error: 142.9084\n",
      "Epoch 5/10\n",
      "50/50 [==============================] - 125s 3s/step - loss: 195.1155 - mean_squared_error: 195.1155 - val_loss: 142.9084 - val_mean_squared_error: 142.9084\n",
      "Epoch 6/10\n",
      "50/50 [==============================] - 125s 3s/step - loss: 306.1377 - mean_squared_error: 306.1377 - val_loss: 142.9084 - val_mean_squared_error: 142.9084\n",
      "Epoch 7/10\n",
      "50/50 [==============================] - 125s 3s/step - loss: 106.7103 - mean_squared_error: 106.7103 - val_loss: 142.9084 - val_mean_squared_error: 142.9084\n",
      "Epoch 8/10\n",
      "50/50 [==============================] - 125s 3s/step - loss: 131.3567 - mean_squared_error: 131.3567 - val_loss: 142.9084 - val_mean_squared_error: 142.9084\n",
      "Epoch 9/10\n",
      "50/50 [==============================] - 122s 2s/step - loss: 175.7063 - mean_squared_error: 175.7063 - val_loss: 142.9084 - val_mean_squared_error: 142.9084\n",
      "Epoch 10/10\n",
      "50/50 [==============================] - 121s 2s/step - loss: 159.7663 - mean_squared_error: 159.7663 - val_loss: 142.9084 - val_mean_squared_error: 142.9084\n"
     ]
    }
   ],
   "source": [
    "with tf.device(\"/CPU:0\"):\n",
    "    model = build_model(vocab_size, embedding_size, pretrained_weights)\n",
    "    model.summary()\n",
    "    model = compile_model(model)\n",
    "    model = fit_model(x_train_pad, y_train, x_val_pad, y_val, model, batch_size=200, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./saved_model/CNN_model/assets\n"
     ]
    }
   ],
   "source": [
    "# Save the entire model as a SavedModel.\n",
    "!mkdir -p saved_model\n",
    "model.save('./saved_model/CNN_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 23s 72ms/step - loss: 142.9084 - mean_squared_error: 142.9084\n"
     ]
    }
   ],
   "source": [
    "(loss, acc) = model.evaluate(x_val_pad, y_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
